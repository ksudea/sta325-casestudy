---
title: "Case Study"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
library(ISLR)
library(leaps)
install.packages("glmnet", repos = "http://cran.us.r-project.org")
library(glmnet)
install.packages("tree", repos = "http://cran.us.r-project.org")
library(pls)
install.packages('fastDummies')
library('fastDummies')
library(mgcv)
```

## Introduction

### The Data

```{r}
data <- read.csv("data/data-train.csv")
data
```


### Goals

Prediction: For a new parameter setting of (Re, F r, St), predict its particle cluster volume distribution in terms of its four raw moments.

Inference: Investigate and interpret how each parameter affects the probability distribution for particle cluster volumes

## Exploratory Data Analysis

First, we will explore the data to ensure it is fit for modelling and determine inital transformations needed of the data, and which model we see would best fit the data.

```{r}
names(data)
summary(data)

```

### Histograms

```{r}
hist(data$R_moment_1)
hist(data$R_moment_2)
hist(data$R_moment_3)
hist(data$R_moment_4)
hist(data$Fr, breaks=20)
hist(data$St, breaks=20)
hist(data$Re, breaks=20)


```

With these histograms its clear to see that each R_moment is heavily right skewed, since there are many rows of 0 in the data. In R_moment_3 and R_moment_4, the maximum values are extremely high whereas the medians are much smaller in comparison, which poses a problem to the analysis. We believe it is best then to apply a transformation to these variables in order to obtain more accurate analysis.


```{r}
hist(log(data$R_moment_1), breaks=20)
hist(log(data$R_moment_2), breaks=20)
hist(log(data$R_moment_3), breaks=20)
hist(log(data$R_moment_4), breaks=20)

```

Performing a log transformation on these variables created more normally distributed variables. While not perfectly normal, this is a big improvement to the non-transformed variables. From here on out, the log version of variables will be used and will be reflected as such in our interpretations and analysis. 

One thing that we should do is turn Fr and Re into ordered, categorical variables, because they only have 2 or 3 unique values each. 




```{r}
pairs(data)
```
It appears that each R_moment variable has somewhat of a linear relationship with St. 

## Initial Modelling

We will fit a basic linear model onto each response variable.

```{r}
model1 <- lm(log(R_moment_1) ~ St + factor(Re) + factor(Fr), data=data)

summary(model1)

model2 <- lm(log(R_moment_2) ~ St + factor(Re) + factor(Fr), data=data)

summary(model2)

model3 <- lm(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data=data)

summary(model3)

model4 <- lm(log(R_moment_4) ~ St + factor(Re) + factor(Fr), data=data)

summary(model4)
```




When all interaction terms are included:

```{r}
glm.full <- lm(cbind(log(R_moment_1), log(R_moment_2), log(R_moment_3), log(R_moment_4)) ~  (St + factor(Re) + factor(Fr))^2, data=data)
summary(glm.full)
```
# Linear modeling 

Trying stepwise selection


```{r}
regfit.bwd <- regsubsets(log(R_moment_1) ~ (St + factor(Re) + factor(Fr)), data = data, nvmax = 10, method = "forward")
summary(regfit.bwd)
#coef(regfit.bwd, which.max(reg.summary.bwd$adjr2))
```

### Trying to see if adding in dummy columns helps with ridge and lasso regression and so forth. feel free to remove this  
```{r}
data <- dummy_cols(data, select_columns = c('Re', 'Fr'))
drop <- c("Re","Fr")
data = data[,!(names(data) %in% drop)]

data
```

# Split data into training and test sets
```{r}
attach(data)
set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
training <- data[train_ind, ]
testing <- data[test_ind_neg, ]
training
testing
```

# Linear model using least squares

```{r}
fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm1 <- predict(fit.lm1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)

fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm2 <- predict(fit.lm2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)

fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm3 <- predict(fit.lm3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)

fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm4 <- predict(fit.lm4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)

mse_test1
mse_test2
mse_test3
mse_test4

```
 

Trying ridge regression and getting errors that number of observations in y is not equal to rows of x


```{r}
train.mat <- model.matrix(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data = training)
test.mat <- model.matrix(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data = testing)
grid <- 10 ^ seq(4, -2, length = 100)
fit.ridge <- glmnet(train.mat, log(training$R_moment_3), alpha = 0, lambda = grid, thresh = 1e-12)
#cv.ridge <- cv.glmnet(train.mat, testing$R_moment_3, alpha = 0, lambda = grid, thresh = 1e-12)
#bestlam.ridge <- cv.ridge$lambda.min
#bestlam.ridge

```

Trying PCR model on the 3rd moment:

```{r}
fit3.pcr <- pcr(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = training, scale = TRUE, validation = "CV")
validationplot(fit3.pcr, val.type = "MSEP")
pred3.pcr <- predict(fit3.pcr, testing, ncomp = 5)
pred3.pcr
mean((pred3.pcr - log(testing$R_moment_3))^2)

```

Same as least squares

Trying PLS on the 4th moment:

```{r}
fit4.pls <- plsr(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = training, validation = "CV")
validationplot(fit4.pls, val.type = "MSEP")
predict4.pls<-predict(fit4.pls,testing,ncomp=5)
mean((predict4.pls - log(testing$R_moment_4))^2)

```
Same as least squares 

# Regression Tree
```{r}
library(tree)
tree1 <- tree(R_moment_1 ~ St + factor(Re) + factor(Fr), data = training)
summary(tree1)
plot(tree1)
text(tree1, pretty = 0)

cv1 <- cv.tree(tree1)
plot(cv1$size, cv1$dev, type = "b")
abline(h = min(cv1$dev) + 1 * sd(cv1$dev), col = "red", lty = 2)

y_hat <- predict(tree1, newdata = testing)
moment1_test <- testing[,"R_moment_1"]
plot(y_hat, moment1_test)
abline(0,1)
test_error <- mean((y_hat-moment1_test)^2)
test_error
tss <- mean((testing$R_moment_1 - mean(testing$R_moment_1))^2)
(rss <- 1 - test_error / tss)
```
# Regrssion tree does not work well on higher moments
```{r}
tree2 <- tree(R_moment_2 ~ St + factor(Re) + factor(Fr), data = training)
summary(tree2)
plot(tree2)
text(tree2, pretty = 0)

cv2 <- cv.tree(tree2)
plot(cv2$size, cv2$dev, type = "b")
abline(h = min(cv2$dev) + 1 * sd(cv2$dev), col = "red", lty = 2)

y_hat <- predict(tree2, newdata = testing)
moment2_test <- testing[,"R_moment_2"]
plot(y_hat, moment2_test)
abline(0,1)
(test_error <- mean((y_hat-moment2_test)^2))
test_error
tss <- mean((testing$R_moment_2 - mean(testing$R_moment_2))^2)
(rss <- 1 - test_error / tss)
```

# Random Forest
```{r}
library(randomForest)
set.seed(120)
rf_mom2 <- randomForest(R_moment_2 ~ St + Re, data = training, ntree = 25,
                           importance = TRUE)
summary(rf_mom2)

yhat_rf <- predict(rf_mom2, newdata = testing)
plot(yhat_rf, moment2_test)
abline(0,1)
mean((yhat_rf - moment2_test)^2)

importance(rf_mom2)
varImpPlot(rf_mom2)
```

# Boosting doesn't work
```{r}
library(gbm)
set.seed(8)
boost2 <- gbm(R_moment_2 ~ St + Re + Fr, data = training,
                      distribution = "gaussian", n.trees = 10000,
                      interaction.depth = 2)
?gbm()
summary(boost2)

# partial independence plots for Price and ShelveLoc
par(mfrow = c(1,3))
plot(boost2, i = "St")

yhat_boost <- predict(boost2, newdata = testing,
                      n.trees = 5000)
mean((yhat_boost - moment2_test)^2)
```

## Box Cox Transformation for Moment 2
```{r}
library(car)
attach(data)
b2 <- boxCox(R_moment_2 ~ St + factor(Re) + factor(Fr))

lambda <- b2$x # lambda values

lik <- b2$y # log likelihood values for SSE

bc2 <- cbind(lambda, lik) # combine lambda and lik

sorted_bc2 <- bc2[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE

head(sorted_bc2, n = 10)
```

```{r}
mod2 <- lm(R_moment_2^(-0.06060606) ~ St + factor(Re) + factor(Fr))
summary(mod2)

plot(mod2$fitted.values,  rstandard(mod2))
```
## Box Cox Transformation for Moment 3
```{r}
b3 <- boxCox(R_moment_3 ~ St + factor(Re) + factor(Fr))

lambda <- b3$x # lambda values

lik <- b3$y # log likelihood values for SSE

bc3 <- cbind(lambda, lik) # combine lambda and lik

sorted_bc3 <- bc3[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE

head(sorted_bc3, n = 10)
```

```{r}
mod3 <- lm(R_moment_3^(-0.06060606) ~ St + factor(Re) + factor(Fr))
summary(mod3)

plot(mod3$fitted.values,  rstandard(mod3))
```

## Box Cox Transformation for Moment 4
```{r}
b4 <- boxCox(R_moment_4 ~ St + factor(Re) + factor(Fr))

lambda <- b4$x # lambda values

lik <- b4$y # log likelihood values for SSE

bc4 <- cbind(lambda, lik) # combine lambda and lik

sorted_bc4 <- bc4[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE

head(sorted_bc4, n = 10)
```

```{r}
mod4 <- lm(R_moment_4^(-0.02020202) ~ St + factor(Re) + factor(Fr))

glm <- glm(R_moment_4 ~ St + factor(Re) + factor(Fr), data = data, family = Gamma("inverse"))
summary(glm)

original <- data[, "R_moment_4"]

after_transformation <- glm$fitted.values

back_transformation <- glm$fitted.values^(1/-0.02020202)

fittedvalues <- data.frame(original, after_transformation, back_transformation)

head(fittedvalues, n = 10)
fittedvalues


y_hat_glm <- predict(glm, newdata = testing)
y_hat_glm
testing$R_moment_4
cbind(y_hat_glm^(1/-.02), testing$R_moment_4)
moment4_test <- testing[,"R_moment_4"]
y_hat_glm
plot(y_hat_glm^(1/-0.02020202), moment4_test)
abline(0,1)
(test_error <- mean((y_hat_glm-moment4_test)^2))
tss <- mean((testing$R_moment_2 - mean(testing$R_moment_2))^2)
(rss <- 1 - test_error / tss)

summary(mod4)

plot(mod4$fitted.values,  rstandard(mod4))
```

# Zero-Inflated Model
```{r}
library(pscl)
data_trunc <- data
attach(data_trunc)
data_trunc$R_moment_1 <- ifelse(R_moment_1 < 0.05, 0, round(R_moment_1, 0))
data_trunc$R_moment_2 <- ifelse(R_moment_2 < 1, 0, round(R_moment_2, 0))
data_trunc$R_moment_3 <- ifelse(R_moment_3 < 10, 0, round(R_moment_3, 0))
data_trunc$R_moment_4 <- ifelse(R_moment_4 < 10^2, 0, round(R_moment_4, 0))
data_trunc
#m1 <- zeroinfl(R_moment_4 ~ St + factor(Re) + factor(Fr), data = data_trunc)
```


## Polynomial Regression

For each of the four moments, we try to fit a polynomial model based on the degree of the numerical variable, St. We also include the other two factored variables in each model.

First moment:
```{r}
polym1 <- lm(log(R_moment_1) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym1)

poly2m1 <- lm(log(R_moment_1) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m1)

poly3m1 <- lm(log(R_moment_1) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m1)

poly4m1 <- lm(log(R_moment_1) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m1)

poly5m1 <- lm(log(R_moment_1) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m1)

poly6m1 <- lm(log(R_moment_1) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m1)

poly7m1 <- lm(log(R_moment_1) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m1)


anova(fit.lm1, polym1, poly2m1, poly3m1, poly4m1, poly5m1, poly6m1, poly7m1)

pred.polym1 <- predict(polym1, testing)
pred.poly2m1 <- predict(poly2m1, testing)
pred.poly3m1 <- predict(poly3m1, testing)
pred.poly4m1 <- predict(poly4m1, testing)
pred.poly5m1 <- predict(poly5m1, testing)
pred.poly6m1 <- predict(poly6m1, testing)
pred.poly7m1 <- predict(poly7m1, testing)

mse_polym1 <- mean((pred.polym1 - log(testing$R_moment_1))^2)
mse_poly2m1 <- mean((pred.poly2m1 - log(testing$R_moment_1))^2)
mse_poly3m1 <- mean((pred.poly3m1 - log(testing$R_moment_1))^2)
mse_poly4m1 <- mean((pred.poly4m1 - log(testing$R_moment_1))^2)
mse_poly5m1 <- mean((pred.poly5m1 - log(testing$R_moment_1))^2)
mse_poly6m1 <- mean((pred.poly6m1 - log(testing$R_moment_1))^2)
mse_poly7m1 <- mean((pred.poly7m1 - log(testing$R_moment_1))^2)

mse_polym1
mse_poly2m1
mse_poly3m1
mse_poly4m1
mse_poly5m1
mse_poly6m1
mse_poly7m1
```
Similar to least squares.

Second moment:
```{r}
polym2 <- lm(log(R_moment_2) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym2)

poly2m2 <- lm(log(R_moment_2) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m2)

poly3m2 <- lm(log(R_moment_2) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m2)

poly4m2 <- lm(log(R_moment_2) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m2)

poly5m2 <- lm(log(R_moment_2) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m2)

poly6m2 <- lm(log(R_moment_2) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m2)

poly7m2 <- lm(log(R_moment_2) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m2)

anova(fit.lm2, polym2, poly2m2, poly3m2, poly4m2, poly5m2, poly6m2, poly7m2)


pred.polym2 <- predict(polym2, testing)
pred.poly2m2 <- predict(poly2m2, testing)
pred.poly3m2 <- predict(poly3m2, testing)
pred.poly4m2 <- predict(poly4m2, testing)
pred.poly5m2 <- predict(poly5m2, testing)
pred.poly6m2 <- predict(poly6m2, testing)
pred.poly7m2 <- predict(poly7m2, testing)


mse_polym2 <- mean((pred.polym2 - log(testing$R_moment_2))^2)
mse_poly2m2 <- mean((pred.poly2m2 - log(testing$R_moment_2))^2)
mse_poly3m2 <- mean((pred.poly3m2 - log(testing$R_moment_2))^2)
mse_poly4m2 <- mean((pred.poly4m2 - log(testing$R_moment_2))^2)
mse_poly5m2 <- mean((pred.poly5m2 - log(testing$R_moment_2))^2)
mse_poly6m2 <- mean((pred.poly6m2 - log(testing$R_moment_2))^2)
mse_poly7m2 <- mean((pred.poly7m2 - log(testing$R_moment_2))^2)

mse_test2
mse_polym2
mse_poly2m2
mse_poly3m2
mse_poly4m2
mse_poly5m2
mse_poly6m2
mse_poly7m2
```
Same as linear regression? Polynomial model with degree 7 has lowest MSE, but degree 5 or LSR may be better based on ANOVA.

Third moment:
```{r}
polym3 <- lm(log(R_moment_3) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym3)

poly2m3 <- lm(log(R_moment_3) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m3)

poly3m3 <- lm(log(R_moment_3) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m3)

poly4m3 <- lm(log(R_moment_3) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m3)

poly5m3 <- lm(log(R_moment_3) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m3)

poly6m3 <- lm(log(R_moment_3) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m3)

poly7m3 <- lm(log(R_moment_3) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m3)

poly8m3 <- lm(log(R_moment_3) ~ poly(St, 9) + factor(Re) + factor(Fr), data = training)
summary(poly8m3)



anova(fit.lm3, polym3, poly2m3, poly3m3, poly4m3, poly5m3, poly6m3, poly7m3, poly8m3)

pred.polym3 <- predict(polym3, testing)
pred.poly2m3 <- predict(poly2m3, testing)
pred.poly3m3 <- predict(poly3m3, testing)
pred.poly4m3 <- predict(poly4m3, testing)
pred.poly5m3 <- predict(poly5m3, testing)
pred.poly6m3 <- predict(poly6m3, testing)
pred.poly7m3 <- predict(poly7m3, testing)
pred.poly8m3 <- predict(poly8m3, testing)


mse_polym3 <- mean((pred.polym3 - log(testing$R_moment_3))^2)
mse_poly2m3 <- mean((pred.poly2m3 - log(testing$R_moment_3))^2)
mse_poly3m3 <- mean((pred.poly3m3 - log(testing$R_moment_3))^2)
mse_poly4m3 <- mean((pred.poly4m3 - log(testing$R_moment_3))^2)
mse_poly5m3 <- mean((pred.poly5m3 - log(testing$R_moment_3))^2)
mse_poly6m3 <- mean((pred.poly6m3 - log(testing$R_moment_3))^2)
mse_poly7m3 <- mean((pred.poly7m3 - log(testing$R_moment_3))^2)
mse_poly8m3 <- mean((pred.poly8m3 - log(testing$R_moment_3))^2)


mse_test3
mse_polym3
mse_poly2m3
mse_poly3m3
mse_poly4m3
mse_poly5m3
mse_poly6m3
mse_poly7m3
mse_poly8m3
```

Seem to be slightly worse than linear regression. Optimal model in terms of MSE still seems to be Least Squares.


Fourth moment:
```{r}
polym4 <- lm(log(R_moment_4) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym4)

poly2m4 <- lm(log(R_moment_4) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m4)

poly3m4 <- lm(log(R_moment_4) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m4)

poly4m4 <- lm(log(R_moment_4) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m4)

poly5m4 <- lm(log(R_moment_4) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m4)

poly6m4 <- lm(log(R_moment_4) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m4)

poly7m4 <- lm(log(R_moment_4) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m4)

poly8m4 <- lm(log(R_moment_4) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly8m4)


anova(fit.lm4, polym4, poly2m4, poly3m4, poly4m4, poly5m4, poly6m4, poly7m4, poly8m4)

pred.polym4 <- predict(polym4, testing)
pred.poly2m4 <- predict(poly2m4, testing)
pred.poly3m4 <- predict(poly3m4, testing)
pred.poly4m4 <- predict(poly4m4, testing)
pred.poly5m4 <- predict(poly5m4, testing)
pred.poly6m4 <- predict(poly6m4, testing)
pred.poly7m4 <- predict(poly7m4, testing)
pred.poly8m4 <- predict(poly8m4, testing)


mse_polym4 <- mean((pred.polym4 - log(testing$R_moment_4))^2)
mse_poly2m4 <- mean((pred.poly2m4 - log(testing$R_moment_4))^2)
mse_poly3m4 <- mean((pred.poly3m4 - log(testing$R_moment_4))^2)
mse_poly4m4 <- mean((pred.poly4m4 - log(testing$R_moment_4))^2)
mse_poly5m4 <- mean((pred.poly5m4 - log(testing$R_moment_4))^2)
mse_poly6m4 <- mean((pred.poly6m4 - log(testing$R_moment_4))^2)
mse_poly7m4 <- mean((pred.poly7m4 - log(testing$R_moment_4))^2)
mse_poly8m4 <- mean((pred.poly8m4 - log(testing$R_moment_4))^2)


mse_test4
mse_polym4
mse_poly2m4
mse_poly3m4
mse_poly4m4
mse_poly5m4
mse_poly6m4
mse_poly7m4
mse_poly8m4
```
The linear regression fit seems to have the minimal MSE for the fourth order.



### Splines
```{r}
library(splines)
```


First moment:
```{r}
spline1 <- lm(log(R_moment_1) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1)
pred.spline1 <- predict(spline1, testing)
mse_spline1 <- mean((pred.spline1 - log(testing$R_moment_1))^2)


spline2 <- lm(log(R_moment_1) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2)
pred.spline2 <- predict(spline2, testing)
mse_spline2 <- mean((pred.spline2 - log(testing$R_moment_1))^2)

spline3 <- lm(log(R_moment_1) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3)
pred.spline3 <- predict(spline3, testing)
mse_spline3 <- mean((pred.spline3 - log(testing$R_moment_1))^2)

spline4 <- lm(log(R_moment_1) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4)
pred.spline4 <- predict(spline4, testing)
mse_spline4 <- mean((pred.spline4 - log(testing$R_moment_1))^2)

spline5 <- lm(log(R_moment_1) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5)
pred.spline5 <- predict(spline5, testing)
mse_spline5 <- mean((pred.spline5 - log(testing$R_moment_1))^2)


mse_spline1
mse_spline2
mse_spline3
mse_spline4
mse_spline5


```

Second moment:
```{r}
spline1m2 <- lm(log(R_moment_2) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m2)
pred.spline1m2 <- predict(spline1m2, testing)
mse_spline1m2 <- mean((pred.spline1m2 - log(testing$R_moment_2))^2)


spline2m2 <- lm(log(R_moment_2) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m2)
pred.spline2m2 <- predict(spline2m2, testing)
mse_spline2m2 <- mean((pred.spline2m2 - log(testing$R_moment_2))^2)

spline3m2 <- lm(log(R_moment_2) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m2)
pred.spline3m2 <- predict(spline3m2, testing)
mse_spline3m2 <- mean((pred.spline3m2 - log(testing$R_moment_2))^2)

spline4m2 <- lm(log(R_moment_2) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m2)
pred.spline4m2 <- predict(spline4m2, testing)
mse_spline4m2 <- mean((pred.spline4m2 - log(testing$R_moment_2))^2)

spline5m2 <- lm(log(R_moment_2) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m2)
pred.spline5m2 <- predict(spline5m2, testing)
mse_spline5m2 <- mean((pred.spline5m2 - log(testing$R_moment_2))^2)

mse_spline1m2
mse_spline2m2
mse_spline3m2
mse_spline4m2
mse_spline5m2

```

Third moment:
```{r}
spline1m3 <- lm(log(R_moment_3) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m3)
pred.spline1m3 <- predict(spline1m3, testing)
mse_spline1m3 <- mean((pred.spline1m3 - log(testing$R_moment_3))^2)


spline2m3 <- lm(log(R_moment_3) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m3)
pred.spline2m3 <- predict(spline2m3, testing)
mse_spline2m3 <- mean((pred.spline2m3 - log(testing$R_moment_3))^2)

spline3m3 <- lm(log(R_moment_3) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m3)
pred.spline3m3 <- predict(spline3m3, testing)
mse_spline3m3 <- mean((pred.spline3m3 - log(testing$R_moment_3))^2)

spline4m3 <- lm(log(R_moment_3) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m3)
pred.spline4m3 <- predict(spline4m3, testing)
mse_spline4m3 <- mean((pred.spline4m3 - log(testing$R_moment_3))^2)

spline5m3 <- lm(log(R_moment_3) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m3)
pred.spline5m3 <- predict(spline5m3, testing)
mse_spline5m3 <- mean((pred.spline5m3 - log(testing$R_moment_3))^2)

mse_spline1m3
mse_spline2m3
mse_spline3m3
mse_spline4m3
mse_spline5m3

```


Fourth moment:
```{r}
spline1m4 <- lm(log(R_moment_4) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m4)
pred.spline1m4 <- predict(spline1m4, testing)
mse_spline1m4 <- mean((pred.spline1m4 - log(testing$R_moment_4))^2)


spline2m4 <- lm(log(R_moment_4) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m4)
pred.spline2m4 <- predict(spline2m4, testing)
mse_spline2m4 <- mean((pred.spline2m4 - log(testing$R_moment_4))^2)

spline3m4 <- lm(log(R_moment_4) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m4)
pred.spline3m4 <- predict(spline3m4, testing)
mse_spline3m4 <- mean((pred.spline3m4 - log(testing$R_moment_4))^2)

spline4m4 <- lm(log(R_moment_4) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m4)
pred.spline4m4 <- predict(spline4m4, testing)
mse_spline4m4 <- mean((pred.spline4m4 - log(testing$R_moment_4))^2)

spline5m4 <- lm(log(R_moment_4) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m4)
pred.spline5m4 <- predict(spline5m4, testing)
mse_spline5m4 <- mean((pred.spline5m4 - log(testing$R_moment_4))^2)

mse_spline1m4
mse_spline2m4
mse_spline3m4
mse_spline4m4
mse_spline5m4
```

## Generalized Additive Model 
```{r}
ftraining <- training
ftesting <- testing
ftraining$Fr <- factor(ftraining$Fr, ordered = TRUE, levels = c(0.052, 0.300, Inf))
ftraining$Re <- factor(ftraining$Re, ordered=TRUE, levels = c(90, 224, 398))
ftesting$Fr <- factor(ftesting$Fr, ordered = TRUE, levels = c(0.052, 0.300, Inf))
ftesting$Re <- factor(ftesting$Re, ordered=TRUE, levels = c(90, 224, 398))

gam.m1 = gam(R_moment_1 ~ s(St) + Re + Fr, data = ftraining)
plot(gam.m1)
summary(gam.m1)
gam.check(gam.m1)

gam.m2 = gam(R_moment_2 ~ s(St) + Re + Fr, data = ftraining)
plot(gam.m2)
summary(gam.m2)
gam.check(gam.m2)

gam.m3 = gam(R_moment_3 ~ s(St) + Re + Fr, data = ftraining)
plot(gam.m3)
summary(gam.m3)
gam.check(gam.m3)

gam.m4 = gam(R_moment_4 ~ s(St) + Re + Fr, data = ftraining)
plot(gam.m4)
summary(gam.m4)
gam.check(gam.m4)

#pred.gam1 <- predict(gam.m1, ftesting)
#pred.gam2 <- predict(gam.m2, ftesting)
#pred.gam3 <- predict(gam.m3, ftesting)
#pred.gam4 <- predict(gam.m4, ftesting)

#mse_gam1 <- mean((pred.gam1 - ftesting$R_moment_1)^2)
#mse_gam2 <- mean((pred.gam2 - ftesting$R_moment_2)^2)
#mse_gam3 <- mean((pred.gam3 - ftesting$R_moment_3)^2)
#mse_gam4 <- mean((pred.gam4 - ftesting$R_moment_4)^2)

#mse_gam1
#mse_gam2
#mse_gam3
#mse_gam4

```


mgcv library takes care of choosing the optimal degrees of freedom for spline models. GAM fitted with a spline on St for each moment. 