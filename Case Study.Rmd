---
title: "Case Study"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
library(ISLR)
library(leaps)
install.packages("glmnet", repos = "http://cran.us.r-project.org")
library(glmnet)
install.packages("tree", repos = "http://cran.us.r-project.org")
library(pls)
install.packages('fastDummies')
library('fastDummies')

```

## Introduction

### The Data

```{r}
data <- read.csv("data/data-train.csv")
data
```


### Goals

Prediction: For a new parameter setting of (Re, F r, St), predict its particle cluster volume distribution in terms of its four raw moments.

Inference: Investigate and interpret how each parameter affects the probability distribution for particle cluster volumes

## Exploratory Data Analysis

First, we will explore the data to ensure it is fit for modelling and determine inital transformations needed of the data, and which model we see would best fit the data.

```{r}
names(data)
summary(data)

```

### Histograms

```{r}
hist(data$R_moment_1)
hist(data$R_moment_2)
hist(data$R_moment_3)
hist(data$R_moment_4)
hist(data$Fr, breaks=20)
hist(data$St, breaks=20)
hist(data$Re, breaks=20)


```

With these histograms its clear to see that each R_moment is heavily right skewed, since there are many rows of 0 in the data. In R_moment_3 and R_moment_4, the maximum values are extremely high whereas the medians are much smaller in comparison, which poses a problem to the analysis. We believe it is best then to apply a transformation to these variables in order to obtain more accurate analysis.


```{r}
hist(log(data$R_moment_1), breaks=20)
hist(log(data$R_moment_2), breaks=20)
hist(log(data$R_moment_3), breaks=20)
hist(log(data$R_moment_4), breaks=20)

```

Performing a log transformation on these variables created more normally distributed variables. While not perfectly normal, this is a big improvement to the non-transformed variables. From here on out, the log version of variables will be used and will be reflected as such in our interpretations and analysis. 

One thing that we should do is turn Fr and Re into ordered, categorical variables, because they only have 2 or 3 unique values each. 

```{r}
#data$Fr <- factor(data$Fr, ordered = TRUE, levels = c(0.052, 0.300, Inf))
#data$Re <- factor(data$Re, ordered=TRUE, levels = c(90, 224, 398))
```


```{r}
pairs(data)
```
It appears that each R_moment variable has somewhat of a linear relationship with St. 

## Initial Modelling

We will fit a basic linear model onto each response variable.

```{r}
model1 <- lm(log(R_moment_1) ~ St + factor(Re) + factor(Fr), data=data)

summary(model1)

model2 <- lm(log(R_moment_2) ~ St + factor(Re) + factor(Fr), data=data)

summary(model2)

model3 <- lm(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data=data)

summary(model3)

model4 <- lm(log(R_moment_4) ~ St + factor(Re) + factor(Fr), data=data)

summary(model4)
```




When all interaction terms are included:

```{r}
glm.full <- lm(cbind(log(R_moment_1), log(R_moment_2), log(R_moment_3), log(R_moment_4)) ~  (St + factor(Re) + factor(Fr))^2, data=data)
summary(glm.full)
```
# Linear modeling 

Trying stepwise selection


```{r}
regfit.bwd <- regsubsets(log(R_moment_1) ~ (St + factor(Re) + factor(Fr)), data = data, nvmax = 10, method = "forward")
summary(regfit.bwd)
#coef(regfit.bwd, which.max(reg.summary.bwd$adjr2))
```

### Trying to see if adding in dummy columns helps with ridge and lasso regression and so forth. feel free to remove this  
```{r}
data <- dummy_cols(data, select_columns = c('Re', 'Fr'))
drop <- c("Re","Fr")
data = data[,!(names(data) %in% drop)]

data
```

# Split data into training and test sets
```{r}
attach(dataf)
set.seed(3)
train_ind <- sample(x = nrow(dataf), size = 0.8 * nrow(dataf))
test_ind_neg <- -train_ind
training <- data[train_ind, ]
testing <- data[test_ind_neg, ]
training
testing
```

# Linear model using least squares

```{r}
fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm1 <- predict(fit.lm1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)

fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm2 <- predict(fit.lm2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)

fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm3 <- predict(fit.lm3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)

fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm4 <- predict(fit.lm4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)

mse_test1
mse_test2
mse_test3
mse_test4

```
 

Trying ridge regression and getting errors that number of observations in y is not equal to rows of x


```{r}
train.mat <- model.matrix(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data = training)
test.mat <- model.matrix(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data = testing)
grid <- 10 ^ seq(4, -2, length = 100)
fit.ridge <- glmnet(train.mat, log(training$R_moment_3), alpha = 0, lambda = grid, thresh = 1e-12)
#cv.ridge <- cv.glmnet(train.mat, testing$R_moment_3, alpha = 0, lambda = grid, thresh = 1e-12)
#bestlam.ridge <- cv.ridge$lambda.min
#bestlam.ridge

```

Trying PCR model on the 3rd moment:

```{r}
fit3.pcr <- pcr(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = training, scale = TRUE, validation = "CV")
validationplot(fit3.pcr, val.type = "MSEP")
pred3.pcr <- predict(fit3.pcr, testing, ncomp = 5)
pred3.pcr
mean((pred3.pcr - log(testing$R_moment_3))^2)

```

Same as least squares

Trying PLS on the 4th moment:

```{r}
fit4.pls <- plsr(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = training, validation = "CV")
validationplot(fit4.pls, val.type = "MSEP")
predict4.pls<-predict(fit4.pls,testing,ncomp=5)
mean((predict4.pls - log(testing$R_moment_4))^2)

```
Same as least squares 

# Regression Tree
```{r}
library(tree)
tree1 <- tree(R_moment_1 ~ St + factor(Re) + factor(Fr), data = training)
summary(tree1)
plot(tree1)
text(tree1, pretty = 0)

cv1 <- cv.tree(tree1)
plot(cv1$size, cv1$dev, type = "b")
abline(h = min(cv1$dev) + 1 * sd(cv1$dev), col = "red", lty = 2)

y_hat <- predict(tree1, newdata = testing)
moment1_test <- testing[,"R_moment_1"]
plot(y_hat, moment1_test)
abline(0,1)
test_error <- mean((y_hat-moment1_test)^2)
tss <- mean((testing$R_moment_1 - mean(testing$R_moment_1))^2)
(rss <- 1 - test_error / tss)
```
# Regrssion tree does not work well on higher moments
```{r}
tree2 <- tree(R_moment_2 ~ St + factor(Re) + factor(Fr), data = training)
summary(tree2)
plot(tree2)
text(tree2, pretty = 0)

cv2 <- cv.tree(tree2)
plot(cv2$size, cv2$dev, type = "b")
abline(h = min(cv2$dev) + 1 * sd(cv2$dev), col = "red", lty = 2)

y_hat <- predict(tree2, newdata = testing)
moment2_test <- testing[,"R_moment_2"]
plot(y_hat, moment2_test)
abline(0,1)
(test_error <- mean((y_hat-moment2_test)^2))
tss <- mean((testing$R_moment_2 - mean(testing$R_moment_2))^2)
(rss <- 1 - test_error / tss)
```

# Random Forest
```{r}
library(randomForest)
set.seed(120)
rf_mom2 <- randomForest(R_moment_2 ~ St + Re, data = training, ntree = 25,
                           importance = TRUE)
summary(rf_mom2)

yhat_rf <- predict(rf_mom2, newdata = testing)
plot(yhat_rf, moment2_test)
abline(0,1)
mean((yhat_rf - moment2_test)^2)

importance(rf_mom2)
varImpPlot(rf_mom2)
```

# Boosting doesn't work
```{r}
library(gbm)
set.seed(8)
boost2 <- gbm(R_moment_2 ~ St + Re + Fr, data = training,
                      distribution = "gaussian", n.trees = 10000,
                      interaction.depth = 2)
?gbm()
summary(boost2)

# partial independence plots for Price and ShelveLoc
par(mfrow = c(1,3))
plot(boost2, i = "St")

yhat_boost <- predict(boost2, newdata = testing,
                      n.trees = 5000)
mean((yhat_boost - moment2_test)^2)
```

## Box Cox Transformation for Moment 2
```{r}
library(car)
attach(data)
b2 <- boxCox(R_moment_2 ~ St + factor(Re) + factor(Fr))

lambda <- b2$x # lambda values

lik <- b2$y # log likelihood values for SSE

bc2 <- cbind(lambda, lik) # combine lambda and lik

sorted_bc2 <- bc2[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE

head(sorted_bc2, n = 10)
```

```{r}
mod2 <- lm(R_moment_2^(-0.06060606) ~ St + factor(Re) + factor(Fr))
summary(mod2)

plot(mod2$fitted.values,  rstandard(mod2))
```
## Box Cox Transformation for Moment 3
```{r}
b3 <- boxCox(R_moment_3 ~ St + factor(Re) + factor(Fr))

lambda <- b3$x # lambda values

lik <- b3$y # log likelihood values for SSE

bc3 <- cbind(lambda, lik) # combine lambda and lik

sorted_bc3 <- bc3[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE

head(sorted_bc3, n = 10)
```

```{r}
mod3 <- lm(R_moment_3^(-0.06060606) ~ St + factor(Re) + factor(Fr))
summary(mod3)

plot(mod3$fitted.values,  rstandard(mod3))
```

## Box Cox Transformation for Moment 4
```{r}
b4 <- boxCox(R_moment_4 ~ St + factor(Re) + factor(Fr))

lambda <- b4$x # lambda values

lik <- b4$y # log likelihood values for SSE

bc4 <- cbind(lambda, lik) # combine lambda and lik

sorted_bc4 <- bc4[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE

head(sorted_bc4, n = 10)
```

```{r}
mod4 <- lm(R_moment_4^(-0.02020202) ~ St + factor(Re) + factor(Fr))

glm <- glm(R_moment_4 ~ St + factor(Re) + factor(Fr), data = data, family = Gamma("inverse"))
summary(glm)

original <- data[, "R_moment_4"]

after_transformation <- glm$fitted.values

back_transformation <- glm$fitted.values^(1/-0.02020202)

fittedvalues <- data.frame(original, after_transformation, back_transformation)

head(fittedvalues, n = 10)
fittedvalues


y_hat_glm <- predict(glm, newdata = testing)
y_hat_glm
testing$R_moment_4
cbind(y_hat_glm^(1/-.02), testing$R_moment_4)
moment4_test <- testing[,"R_moment_4"]
y_hat_glm
plot(y_hat_glm^(1/-0.02020202), moment4_test)
abline(0,1)
(test_error <- mean((y_hat_glm-moment4_test)^2))
tss <- mean((testing$R_moment_2 - mean(testing$R_moment_2))^2)
(rss <- 1 - test_error / tss)

summary(mod4)

plot(mod4$fitted.values,  rstandard(mod4))
```

# Zero-Inflated Model
```{r}
library(pscl)
data_trunc <- data
attach(data_trunc)
data_trunc$R_moment_1 <- ifelse(R_moment_1 < 0.05, 0, round(R_moment_1, 0))
data_trunc$R_moment_2 <- ifelse(R_moment_2 < 1, 0, round(R_moment_2, 0))
data_trunc$R_moment_3 <- ifelse(R_moment_3 < 10, 0, round(R_moment_3, 0))
data_trunc$R_moment_4 <- ifelse(R_moment_4 < 10^2, 0, round(R_moment_4, 0))
data_trunc
#m1 <- zeroinfl(R_moment_4 ~ St + factor(Re) + factor(Fr), data = data_trunc)
```

