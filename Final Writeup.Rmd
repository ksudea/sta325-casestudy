---
title: "Final Writeup"
author: "Kadriye Sude Almus, Cheyenne Kim, Martin Lim, Carrie Wang, Michael Li"
date: "10/25/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
library(ISLR)
library(leaps)
install.packages("glmnet", repos = "http://cran.us.r-project.org")
library(glmnet)
install.packages("tree", repos = "http://cran.us.r-project.org")
library(pls)
install.packages('fastDummies')
library('fastDummies')
library(mgcv)
library(car)
```

### The Data

```{r}
data <- read.csv("data/data-train.csv")
```

## Exploratory Data Analysis

First, we will explore the data to ensure it is fit for modelling and determine inital transformations needed of the data, and which model we see would best fit the data.

```{r}
names(data)
summary(data)

```

### Histograms

```{r}
hist(data$R_moment_1)
hist(data$R_moment_2)
hist(data$R_moment_3)
hist(data$R_moment_4)
hist(data$Fr, breaks=20)
hist(data$St, breaks=20)
hist(data$Re, breaks=20)


```

With these histograms its clear to see that each R_moment is heavily right skewed, since there are many rows of 0 in the data. In R_moment_3 and R_moment_4, the maximum values are extremely high whereas the medians are much smaller in comparison, which poses a problem to the analysis. We believe it is best then to apply a transformation to these variables in order to obtain more accurate analysis.


```{r}
hist(log(data$R_moment_1), breaks=20)
hist(log(data$R_moment_2), breaks=20)
hist(log(data$R_moment_3), breaks=20)
hist(log(data$R_moment_4), breaks=20)

```

Performing a log transformation on these variables created more normally distributed variables. While not perfectly normal, this is a big improvement to the non-transformed variables. From here on out, the log version of variables will be used and will be reflected as such in our interpretations and analysis. 

One thing that we should do is turn Fr and Re into ordered, categorical variables, because they only have 2 or 3 unique values each. 




```{r}
pairs(data)
```
It appears that each R_moment variable has somewhat of a linear relationship with St. 

## Initial Modelling

We will fit a basic linear model onto each log-transformed response variable.

```{r}
model1 <- lm(log(R_moment_1) ~ St + factor(Re) + factor(Fr), data=data)

summary(model1)

model2 <- lm(log(R_moment_2) ~ St + factor(Re) + factor(Fr), data=data)

summary(model2)

model3 <- lm(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data=data)

summary(model3)

model4 <- lm(log(R_moment_4) ~ St + factor(Re) + factor(Fr), data=data)

summary(model4)
```


Exploring collinearity:

```{r}
vif(model1)
vif(model2)
vif(model3)
vif(model4)

```

When all interaction terms are included:

```{r}
glm.full <- lm(cbind(log(R_moment_1), log(R_moment_2), log(R_moment_3), log(R_moment_4)) ~  (St + factor(Re) + factor(Fr))^2, data=data)
summary(glm.full)
```

Re and Fr seem to have significant interaction for all moments, while St and Re only have significant interaction for the first moment. We will attempt to only include the interaction term for Re and Fr.

A model with the interaction term for Re and Fr:

```{r}
glm.inter <- lm(cbind(log(R_moment_1), log(R_moment_2), log(R_moment_3), log(R_moment_4)) ~  (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data=data)
summary(glm.inter)

```

Adding the interaction term between Re and Fr improved the fit of the model according to the adjusted R^2 values. 

# Split data into training and test sets
```{r}
attach(data)
set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
training <- data[train_ind, ]
testing <- data[test_ind_neg, ]
ftraining <- training
ftesting <- testing
ftraining$Fr <- factor(ftraining$Fr, levels = c(0.052, 0.300, Inf))
ftraining$Re <- factor(ftraining$Re, levels = c(90, 224, 398))
ftesting$Fr <- factor(ftesting$Fr, levels = c(0.052, 0.300, Inf))
ftesting$Re <- factor(ftesting$Re, levels = c(90, 224, 398))


```


# Linear model using least squares & no interaction term 

```{r}
fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm1 <- predict(fit.lm1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)

fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm2 <- predict(fit.lm2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)

fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm3 <- predict(fit.lm3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)

fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm4 <- predict(fit.lm4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)

mse_test1
mse_test2
mse_test3
mse_test4

```

# Linear model using least squares & interaction term

```{r}

fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm1 <- predict(fit.lm1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)

fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm2 <- predict(fit.lm2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)

fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm3 <- predict(fit.lm3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)

fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm4 <- predict(fit.lm4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)

mse_test1
mse_test2
mse_test3
mse_test4

```
 
Having an interaction term significantly improved the test MSEs of the linear model.

### Other Linear Regularization Techniques

We would now like to explore linear model regularization techniques on the higher moments to see if any produce a better adj. R^2 or test MSE value than the least squares with an interaction term. 

Trying PCR model on the 3rd moment:

```{r}
fit3.pcr <- pcr(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = training, scale = TRUE, validation = "CV")
validationplot(fit3.pcr, val.type = "MSEP")
pred3.pcr <- predict(fit3.pcr, testing, ncomp = 5)
mean((pred3.pcr - log(testing$R_moment_3))^2)

```

The test MSE is the same as the least squares linear model with no interaction term. It has a higher test MSE and lower adj. R^2  than the linear model with the interaction term added.

Trying PLS on the 4th moment:

```{r}
fit4.pls <- plsr(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = training, validation = "CV")
validationplot(fit4.pls, val.type = "MSEP")
predict4.pls<-predict(fit4.pls,testing,ncomp=5)
mean((predict4.pls - log(testing$R_moment_4))^2)

```

The test MSE is the same as the least squares linear model with no interaction term. It has a higher test MSE and lower adj. R^2  than the linear model with the interaction term added.

## Nonlinear Techniques

For ease of perusal, we have inserted our unused/ineffective model selection techniques below as plain text instead of R code. 

### Regression Tree
library(tree)
tree1 <- tree(R_moment_1 ~ St + factor(Re) + factor(Fr), data = training)

cv1 <- cv.tree(tree1)
plot(cv1$size, cv1$dev, type = "b")
abline(h = min(cv1$dev) + 1 * sd(cv1$dev), col = "red", lty = 2)

y_hat <- predict(tree1, newdata = testing)
moment1_test <- testing[,"R_moment_1"]
plot(y_hat, moment1_test)
abline(0,1)
test_error <- mean((y_hat-moment1_test)^2)
test_error
tss <- mean((testing$R_moment_1 - mean(testing$R_moment_1))^2)
(rss <- 1 - test_error / tss)
### Regrssion tree does not work well on higher moments
tree2 <- tree(R_moment_2 ~ St + factor(Re) + factor(Fr), data = training)
summary(tree2)
plot(tree2)
text(tree2, pretty = 0)

cv2 <- cv.tree(tree2)
plot(cv2$size, cv2$dev, type = "b")
abline(h = min(cv2$dev) + 1 * sd(cv2$dev), col = "red", lty = 2)

y_hat <- predict(tree2, newdata = testing)
moment2_test <- testing[,"R_moment_2"]
plot(y_hat, moment2_test)
abline(0,1)
(test_error <- mean((y_hat-moment2_test)^2))
test_error
tss <- mean((testing$R_moment_2 - mean(testing$R_moment_2))^2)
(rss <- 1 - test_error / tss)

# Random Forest
library(randomForest)
set.seed(120)
rf_mom2 <- randomForest(R_moment_2 ~ St + Re, data = training, ntree = 25,
                           importance = TRUE)
summary(rf_mom2)

yhat_rf <- predict(rf_mom2, newdata = testing)
plot(yhat_rf, moment2_test)
abline(0,1)
mean((yhat_rf - moment2_test)^2)

importance(rf_mom2)
varImpPlot(rf_mom2)

For each of the four moments, we try to fit a polynomial model based on the degree of the numerical variable, St. We also include the other two factored variables in each model.

First moment:

polym1 <- lm(log(R_moment_1) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)

poly2m1 <- lm(log(R_moment_1) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)

poly3m1 <- lm(log(R_moment_1) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)

poly4m1 <- lm(log(R_moment_1) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)

poly5m1 <- lm(log(R_moment_1) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)

poly6m1 <- lm(log(R_moment_1) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)

poly7m1 <- lm(log(R_moment_1) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)


anova(fit.lm1, polym1, poly2m1, poly3m1, poly4m1, poly5m1, poly6m1, poly7m1)

pred.polym1 <- predict(polym1, testing)
pred.poly2m1 <- predict(poly2m1, testing)
pred.poly3m1 <- predict(poly3m1, testing)
pred.poly4m1 <- predict(poly4m1, testing)
pred.poly5m1 <- predict(poly5m1, testing)
pred.poly6m1 <- predict(poly6m1, testing)
pred.poly7m1 <- predict(poly7m1, testing)

mse_polym1 <- mean((pred.polym1 - log(testing$R_moment_1))^2)
mse_poly2m1 <- mean((pred.poly2m1 - log(testing$R_moment_1))^2)
mse_poly3m1 <- mean((pred.poly3m1 - log(testing$R_moment_1))^2)
mse_poly4m1 <- mean((pred.poly4m1 - log(testing$R_moment_1))^2)
mse_poly5m1 <- mean((pred.poly5m1 - log(testing$R_moment_1))^2)
mse_poly6m1 <- mean((pred.poly6m1 - log(testing$R_moment_1))^2)
mse_poly7m1 <- mean((pred.poly7m1 - log(testing$R_moment_1))^2)

mse_polym1
mse_poly2m1
mse_poly3m1
mse_poly4m1
mse_poly5m1
mse_poly6m1
mse_poly7m1

Similar to least squares.

Second moment:
polym2 <- lm(log(R_moment_2) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
poly2m2 <- lm(log(R_moment_2) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)

poly3m2 <- lm(log(R_moment_2) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)

poly4m2 <- lm(log(R_moment_2) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)

poly5m2 <- lm(log(R_moment_2) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)

poly6m2 <- lm(log(R_moment_2) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)

poly7m2 <- lm(log(R_moment_2) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)

anova(fit.lm2, polym2, poly2m2, poly3m2, poly4m2, poly5m2, poly6m2, poly7m2)


pred.polym2 <- predict(polym2, testing)
pred.poly2m2 <- predict(poly2m2, testing)
pred.poly3m2 <- predict(poly3m2, testing)
pred.poly4m2 <- predict(poly4m2, testing)
pred.poly5m2 <- predict(poly5m2, testing)
pred.poly6m2 <- predict(poly6m2, testing)
pred.poly7m2 <- predict(poly7m2, testing)


mse_polym2 <- mean((pred.polym2 - log(testing$R_moment_2))^2)
mse_poly2m2 <- mean((pred.poly2m2 - log(testing$R_moment_2))^2)
mse_poly3m2 <- mean((pred.poly3m2 - log(testing$R_moment_2))^2)
mse_poly4m2 <- mean((pred.poly4m2 - log(testing$R_moment_2))^2)
mse_poly5m2 <- mean((pred.poly5m2 - log(testing$R_moment_2))^2)
mse_poly6m2 <- mean((pred.poly6m2 - log(testing$R_moment_2))^2)
mse_poly7m2 <- mean((pred.poly7m2 - log(testing$R_moment_2))^2)

mse_test2
mse_polym2
mse_poly2m2
mse_poly3m2
mse_poly4m2
mse_poly5m2
mse_poly6m2
mse_poly7m2
Same as linear regression? Polynomial model with degree 7 has lowest MSE, but degree 5 or LSR may be better based on ANOVA.

Third moment:
polym3 <- lm(log(R_moment_3) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)

poly2m3 <- lm(log(R_moment_3) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)

poly3m3 <- lm(log(R_moment_3) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)

poly4m3 <- lm(log(R_moment_3) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)

poly5m3 <- lm(log(R_moment_3) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)

poly6m3 <- lm(log(R_moment_3) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)

poly7m3 <- lm(log(R_moment_3) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)

poly8m3 <- lm(log(R_moment_3) ~ poly(St, 9) + factor(Re) + factor(Fr), data = training)

anova(fit.lm3, polym3, poly2m3, poly3m3, poly4m3, poly5m3, poly6m3, poly7m3, poly8m3)

pred.polym3 <- predict(polym3, testing)
pred.poly2m3 <- predict(poly2m3, testing)
pred.poly3m3 <- predict(poly3m3, testing)
pred.poly4m3 <- predict(poly4m3, testing)
pred.poly5m3 <- predict(poly5m3, testing)
pred.poly6m3 <- predict(poly6m3, testing)
pred.poly7m3 <- predict(poly7m3, testing)
pred.poly8m3 <- predict(poly8m3, testing)


mse_polym3 <- mean((pred.polym3 - log(testing$R_moment_3))^2)
mse_poly2m3 <- mean((pred.poly2m3 - log(testing$R_moment_3))^2)
mse_poly3m3 <- mean((pred.poly3m3 - log(testing$R_moment_3))^2)
mse_poly4m3 <- mean((pred.poly4m3 - log(testing$R_moment_3))^2)
mse_poly5m3 <- mean((pred.poly5m3 - log(testing$R_moment_3))^2)
mse_poly6m3 <- mean((pred.poly6m3 - log(testing$R_moment_3))^2)
mse_poly7m3 <- mean((pred.poly7m3 - log(testing$R_moment_3))^2)
mse_poly8m3 <- mean((pred.poly8m3 - log(testing$R_moment_3))^2)


mse_test3
mse_polym3
mse_poly2m3
mse_poly3m3
mse_poly4m3
mse_poly5m3
mse_poly6m3
mse_poly7m3
mse_poly8m3

Seem to be slightly worse than linear regression. Optimal model in terms of MSE still seems to be Least Squares.


Fourth moment:
polym4 <- lm(log(R_moment_4) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)

poly2m4 <- lm(log(R_moment_4) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)

poly3m4 <- lm(log(R_moment_4) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)

poly4m4 <- lm(log(R_moment_4) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)

poly5m4 <- lm(log(R_moment_4) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)

poly6m4 <- lm(log(R_moment_4) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)

poly7m4 <- lm(log(R_moment_4) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)

poly8m4 <- lm(log(R_moment_4) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)


anova(fit.lm4, polym4, poly2m4, poly3m4, poly4m4, poly5m4, poly6m4, poly7m4, poly8m4)

pred.polym4 <- predict(polym4, testing)
pred.poly2m4 <- predict(poly2m4, testing)
pred.poly3m4 <- predict(poly3m4, testing)
pred.poly4m4 <- predict(poly4m4, testing)
pred.poly5m4 <- predict(poly5m4, testing)
pred.poly6m4 <- predict(poly6m4, testing)
pred.poly7m4 <- predict(poly7m4, testing)
pred.poly8m4 <- predict(poly8m4, testing)


mse_polym4 <- mean((pred.polym4 - log(testing$R_moment_4))^2)
mse_poly2m4 <- mean((pred.poly2m4 - log(testing$R_moment_4))^2)
mse_poly3m4 <- mean((pred.poly3m4 - log(testing$R_moment_4))^2)
mse_poly4m4 <- mean((pred.poly4m4 - log(testing$R_moment_4))^2)
mse_poly5m4 <- mean((pred.poly5m4 - log(testing$R_moment_4))^2)
mse_poly6m4 <- mean((pred.poly6m4 - log(testing$R_moment_4))^2)
mse_poly7m4 <- mean((pred.poly7m4 - log(testing$R_moment_4))^2)
mse_poly8m4 <- mean((pred.poly8m4 - log(testing$R_moment_4))^2)


mse_test4
mse_polym4
mse_poly2m4
mse_poly3m4
mse_poly4m4
mse_poly5m4
mse_poly6m4
mse_poly7m4
mse_poly8m4
The linear regression fit seems to have the minimal MSE for the fourth order.


Attempting splines:

library(splines)


First moment:
spline1 <- lm(log(R_moment_1) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
pred.spline1 <- predict(spline1, testing)
mse_spline1 <- mean((pred.spline1 - log(testing$R_moment_1))^2)


spline2 <- lm(log(R_moment_1) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
pred.spline2 <- predict(spline2, testing)
mse_spline2 <- mean((pred.spline2 - log(testing$R_moment_1))^2)

spline3 <- lm(log(R_moment_1) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
pred.spline3 <- predict(spline3, testing)
mse_spline3 <- mean((pred.spline3 - log(testing$R_moment_1))^2)

spline4 <- lm(log(R_moment_1) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
pred.spline4 <- predict(spline4, testing)
mse_spline4 <- mean((pred.spline4 - log(testing$R_moment_1))^2)

spline5 <- lm(log(R_moment_1) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
pred.spline5 <- predict(spline5, testing)
mse_spline5 <- mean((pred.spline5 - log(testing$R_moment_1))^2)


mse_spline1
mse_spline2
mse_spline3
mse_spline4
mse_spline5



Second moment:

spline1m2 <- lm(log(R_moment_2) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m2)
pred.spline1m2 <- predict(spline1m2, testing)
mse_spline1m2 <- mean((pred.spline1m2 - log(testing$R_moment_2))^2)


spline2m2 <- lm(log(R_moment_2) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m2)
pred.spline2m2 <- predict(spline2m2, testing)
mse_spline2m2 <- mean((pred.spline2m2 - log(testing$R_moment_2))^2)

spline3m2 <- lm(log(R_moment_2) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m2)
pred.spline3m2 <- predict(spline3m2, testing)
mse_spline3m2 <- mean((pred.spline3m2 - log(testing$R_moment_2))^2)

spline4m2 <- lm(log(R_moment_2) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m2)
pred.spline4m2 <- predict(spline4m2, testing)
mse_spline4m2 <- mean((pred.spline4m2 - log(testing$R_moment_2))^2)

spline5m2 <- lm(log(R_moment_2) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m2)
pred.spline5m2 <- predict(spline5m2, testing)
mse_spline5m2 <- mean((pred.spline5m2 - log(testing$R_moment_2))^2)

mse_spline1m2
mse_spline2m2
mse_spline3m2
mse_spline4m2
mse_spline5m2


Third moment:

spline1m3 <- lm(log(R_moment_3) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m3)
pred.spline1m3 <- predict(spline1m3, testing)
mse_spline1m3 <- mean((pred.spline1m3 - log(testing$R_moment_3))^2)


spline2m3 <- lm(log(R_moment_3) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m3)
pred.spline2m3 <- predict(spline2m3, testing)
mse_spline2m3 <- mean((pred.spline2m3 - log(testing$R_moment_3))^2)

spline3m3 <- lm(log(R_moment_3) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m3)
pred.spline3m3 <- predict(spline3m3, testing)
mse_spline3m3 <- mean((pred.spline3m3 - log(testing$R_moment_3))^2)

spline4m3 <- lm(log(R_moment_3) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m3)
pred.spline4m3 <- predict(spline4m3, testing)
mse_spline4m3 <- mean((pred.spline4m3 - log(testing$R_moment_3))^2)

spline5m3 <- lm(log(R_moment_3) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m3)
pred.spline5m3 <- predict(spline5m3, testing)
mse_spline5m3 <- mean((pred.spline5m3 - log(testing$R_moment_3))^2)

mse_spline1m3
mse_spline2m3
mse_spline3m3
mse_spline4m3
mse_spline5m3



Fourth moment:
spline1m4 <- lm(log(R_moment_4) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
pred.spline1m4 <- predict(spline1m4, testing)
mse_spline1m4 <- mean((pred.spline1m4 - log(testing$R_moment_4))^2)


spline2m4 <- lm(log(R_moment_4) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
pred.spline2m4 <- predict(spline2m4, testing)
mse_spline2m4 <- mean((pred.spline2m4 - log(testing$R_moment_4))^2)

spline3m4 <- lm(log(R_moment_4) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
pred.spline3m4 <- predict(spline3m4, testing)
mse_spline3m4 <- mean((pred.spline3m4 - log(testing$R_moment_4))^2)

spline4m4 <- lm(log(R_moment_4) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
pred.spline4m4 <- predict(spline4m4, testing)
mse_spline4m4 <- mean((pred.spline4m4 - log(testing$R_moment_4))^2)

spline5m4 <- lm(log(R_moment_4) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
pred.spline5m4 <- predict(spline5m4, testing)
mse_spline5m4 <- mean((pred.spline5m4 - log(testing$R_moment_4))^2)

mse_spline1m4
mse_spline2m4
mse_spline3m4
mse_spline4m4
mse_spline5m4

The Generalized Additive Model with a spline on St is shown below. 


# Final Model and Predictions



```{r}
fit.lm1 <- lm(log(R_moment_1) ~ (St + Re + Fr + Re*Fr), data = ftraining)
pred.lm1 <- predict(fit.lm1, ftesting)
plot(fit.lm1)
summary(fit.lm1)

mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)
mse_test1

gam.m2 = gam(log(R_moment_2) ~ s(St) + Re + Fr + St:Re + St:Fr + Re:Fr, data = ftraining)
plot(gam.m2)
summary(gam.m2)
gam.check(gam.m2)

gam.m3 = gam(log(R_moment_3) ~ s(St) + Re + Fr + St:Re + St:Fr + Re:Fr, data = ftraining)
plot(gam.m3)
summary(gam.m3)
gam.check(gam.m3)

gam.m4 = gam(log(R_moment_4) ~ s(St) + Re + Fr + St:Re + St:Fr + Re:Fr, data = ftraining)
plot(gam.m4)
summary(gam.m4)
gam.check(gam.m4)

pred.gam2 <- predict(gam.m2, ftesting)
pred.gam3 <- predict(gam.m3, ftesting)
pred.gam4 <- predict(gam.m4, ftesting)

mse_gam2 <- mean((pred.gam2 - log(ftesting$R_moment_2))^2)
mse_gam3 <- mean((pred.gam3 - log(ftesting$R_moment_3))^2)
mse_gam4 <- mean((pred.gam4 - log(ftesting$R_moment_4))^2)

mse_gam2
mse_gam3
mse_gam4

```


### Predictions on the Test Data
```{r}
datat <- read.csv("data/data-test.csv")
datat$Fr <- factor(datat$Fr, levels = c(0.052, 0.300, Inf))
datat$Re <- factor(datat$Re, levels = c(90, 224, 398))

```

<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
>>>>>>> 0876929e6baca1821fce81555debda29e03c24d6
>>>>>>> 23bda2f8d40d0f1915d8720dc0f4dcc32f20b921
```{r}

predt.lm1 <- predict(fit.lm1, datat)
predt.gam2 <- predict(gam.m2, datat)
predt.gam3 <- predict(gam.m3, datat)
predt.gam4 <- predict(gam.m4, datat)

```

<<<<<<< HEAD
### Prediction Consolidation
=======
<<<<<<< HEAD
### Prediction Consolidation
=======
### Final Model
>>>>>>> 6b49e611164cfe92b2e104051e6236aa76c55b9c
>>>>>>> 0876929e6baca1821fce81555debda29e03c24d6
>>>>>>> 23bda2f8d40d0f1915d8720dc0f4dcc32f20b921

Creating a csv of predictions:


```{r}
dataframe_all <- bind_rows(predt.lm1, predt.gam2, predt.gam3, predt.gam4)
dataframe_all <- exp(dataframe_all)
dataframe_all
write.csv(dataframe_all,"data/predictions.csv", row.names = FALSE)


```
